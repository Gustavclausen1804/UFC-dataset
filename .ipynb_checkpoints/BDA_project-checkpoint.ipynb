{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### READ ME\n",
    "This is a final project for the course 'Business Data Analytics, Quantitative Methods and Visualization' (2021, Copenhagen Business School).\n",
    "The goal of this project is to be able to predict the outcome of a certain UFC fight, more specifically to predict whether the Red or Blue fighter is more likely to win the match.\n",
    "\n",
    "We are going to do this through the following steps:\n",
    "1. __Importing necessary libraries and tools.__\n",
    "2. __Loading and cleaning the dataset.__\n",
    "3. __Exploratory Data Analysis.__\n",
    "4. __Data pre-processing.__\n",
    "5. __Machine Learning Models.__\n",
    "6. __Making a prediction.__\n",
    "7. __Showcasing results.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Legend\n",
    "\n",
    "- ufc = the original dataset\n",
    "- column_names = list of column names\n",
    "- ufc_df = the dataset without NaNs\n",
    "- datatypes = dictionary with column names + their datatypes\n",
    "- ufc_ohe = one hot encoded dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing necessary libraries and tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "\n",
    "import pandas as pd #used\n",
    "import matplotlib.pyplot as plt #used\n",
    "import numpy as np #used\n",
    "import seaborn as sns #used\n",
    "\n",
    "# models\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier #used\n",
    "from sklearn.neural_network import MLPClassifier #used\n",
    "from sklearn.tree import DecisionTreeClassifier #used\n",
    "from sklearn.ensemble import RandomForestClassifier #used\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler #used\n",
    "from sklearn.feature_selection import SelectKBest #used\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# use cross validation with grid search as well. \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# other tools\n",
    "\n",
    "from sklearn.model_selection import train_test_split #used\n",
    "from sklearn.metrics import accuracy_score #used\n",
    "\n",
    "import warnings #used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading and cleaning the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load excel file (ufc.xlsx) and print out the first couple rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ufc = pd.read_excel('ufc.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Get number of rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of rows:', ufc.shape[0])\n",
    "print('Number of columns:', ufc.shape[1])\n",
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Get the list of column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ufc.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *We can call this list whenever we want to check the name of a column.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Check for missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ufc.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As we can see, there are some NaNs in these columns that should later be taken care of:\n",
    "- B_avg_SIG_STR_landed\n",
    "- B_avg_SIG_STR_pct\n",
    "- B_avg_SUB_ATT     \n",
    "- B_avg_TD_landed      \n",
    "- B_avg_TD_pct\n",
    "- R_avg_SIG_STR_landed\n",
    "- R_avg_SIG_STR_pct\n",
    "- R_avg_SUB_ATT     \n",
    "- R_avg_TD_landed      \n",
    "- R_avg_TD_pct  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Check for duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = ufc.duplicated()\n",
    "ufc[duplicates]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *It seems that there are no duplicates, therefore, we only need to focus on NaN values.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Delete rows with at least 1 missing value (NaN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ufc_df = ufc.dropna()\n",
    "\n",
    "print('Number of rows:', ufc_df.shape[0])\n",
    "print('Number of columns:', ufc_df.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *1062 rows got dropped.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Check if columns have the right datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatypes = ufc_df.dtypes.to_dict()\n",
    "datatypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- O = object\n",
    "- int64 = integer\n",
    "- float64 = float\n",
    "- <M8[ns] = date (\"On a machine whose byte order is little endian, there is no difference between *np.dtype('datetime64[ns]')* and *np.dtype('<M8[ns]')*\")\n",
    "- bool = True/False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Fix datatypes if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Use info() again to check missing values and datatypes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Exploratory Data Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Use describe() for a better overview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ufc_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Pre-processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Drop unnecessary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop date too\n",
    "ufc_df = ufc_df.drop(labels = 'date', axis = 1)\n",
    "\n",
    "# As we are going to decide who the winner is based on the stats of the fighters, we don't need their names\n",
    "ufc_df = ufc_df.drop(labels = ['R_fighter', 'B_fighter'], axis = 1)\n",
    "\n",
    "\n",
    "# After creating the 2 columns for EU-odds, we don't need the US ones\n",
    "ufc_df = ufc_df.drop(labels = ['R_odds_moneyline', 'B_odds_moneyline'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Deal with nominal features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "onehot_columns = ['location', 'country', 'weight_class', 'gender', 'R_Stance', 'B_Stance']\n",
    "\n",
    "ufc_ohe = pd.get_dummies(ufc_df, columns = onehot_columns, drop_first=True)\n",
    "ufc_ohe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Set data (X) and target (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ufc_ohe.loc[:, ufc_ohe.columns != 'Winner']\n",
    "y = ufc_ohe['Winner']\n",
    "\n",
    "print('X shape:', X.shape)\n",
    "print('y shape:', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Get train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Deal with numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_columns = ['R_odds_decimal', 'B_odds_decimal', 'R_ev', 'B_ev', 'no_of_rounds', 'B_current_lose_streak', 'B_current_win_streak', \n",
    "                'B_draw', 'B_avg_SIG_STR_landed', 'B_avg_SIG_STR_pct', 'B_avg_SUB_ATT', 'B_avg_TD_landed', 'B_avg_TD_pct', \n",
    "                'B_longest_win_streak', 'B_losses', 'B_total_rounds_fought', 'B_total_title_bouts', 'B_win_by_Decision_Majority',\n",
    "               'B_win_by_Decision_Split', 'B_win_by_Decision_Unanimous', 'B_win_by_KO/TKO', 'B_win_by_Submission',\n",
    "                'B_win_by_TKO_Doctor_Stoppage', 'B_wins', 'B_Height_cms', 'B_Reach_cms', 'B_Weight_lbs', 'R_current_win_streak', \n",
    "                'R_draw', 'R_avg_SIG_STR_landed', 'R_avg_SIG_STR_pct', 'R_avg_SUB_ATT', 'R_avg_TD_landed', 'R_avg_TD_pct', \n",
    "                'R_longest_win_streak', 'R_losses', 'R_total_rounds_fought', 'R_total_title_bouts', 'R_win_by_Decision_Majority',\n",
    "               'R_win_by_Decision_Split', 'R_win_by_Decision_Unanimous', 'R_win_by_KO/TKO', 'R_win_by_Submission',\n",
    "                'R_win_by_TKO_Doctor_Stoppage', 'R_wins', 'R_Height_cms', 'R_Reach_cms', 'R_Weight_lbs', 'R_age', 'B_age',\n",
    "                'lose_streak_dif', 'win_streak_dif', 'longest_win_streak_dif', 'win_dif', 'loss_dif', 'total_round_dif',\n",
    "               'total_title_bout_dif', 'ko_dif', 'sub_dif', 'height_dif', 'reach_dif', 'age_dif', 'sig_str_dif', 'avg_sub_att_dif',\n",
    "               'avg_td_dif']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train[scale_columns] = scaler.fit_transform(X_train[scale_columns])\n",
    "\n",
    "X_test[scale_columns] = scaler.transform(X_test[scale_columns])\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Show correlations on a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation heatmap\n",
    "numerical_features_with_target = ['Winner'] + scale_columns\n",
    "corr = ufc_ohe[numerical_features_with_target].corr(method='pearson')\n",
    "\n",
    "f, ax = plt.subplots(figsize=(16, 12))\n",
    "sns.heatmap(corr, square= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Select the 20 most influential features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting best features\n",
    "print(X.shape)\n",
    "\n",
    "k_best = SelectKBest(k = 20)\n",
    "k_best.fit(X_train, y_train)\n",
    "X_train_k_best = k_best.transform(X_train)\n",
    "X_test_k_best = k_best.transform(X_test)\n",
    "\n",
    "print(X_train_k_best.shape)\n",
    "print(X.columns[k_best.get_support()])\n",
    "\n",
    "best_features = X.columns[k_best.get_support()]\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 Set X but only with the best features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ufc_ohe[best_features]\n",
    "y = ufc_ohe['Winner']\n",
    "\n",
    "print('X shape:', X.shape)\n",
    "print('y shape:', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Machine Learning Models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Build models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# neural network\n",
    "mpl = MLPClassifier()\n",
    "\n",
    "# decision tree\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "# random forest\n",
    "forest = RandomForestClassifier()\n",
    "\n",
    "classifiers = (knn, mpl, tree, forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Build the 'applyModel' function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyModel(model,name,X_train, y_train, X_test, y_test):\n",
    "    m = model.fit(X_train,y_train)\n",
    "    print(name, '- Training accuracy:', m.score(X_train, y_train))\n",
    "    print(name, '- Testing accuracy:', m.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Train and test with the different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in classifiers:\n",
    "    n = str(c)\n",
    "    applyModel(c, n, X_train, y_train, X_test, y_test)\n",
    "    print('')\n",
    "    \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Play around with KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_accuracy = []\n",
    "testing_accuracy = []\n",
    "number_of_neighbors =[]\n",
    "weighting_choice = []\n",
    "\n",
    "weight_values = ['distance', 'uniform']\n",
    "\n",
    "for n_neighbors in range(1,100):\n",
    "    for weights in weight_values:\n",
    "        clf = KNeighborsClassifier(n_neighbors = n_neighbors, weights = weights)\n",
    "        clf.fit(X_train, y_train)\n",
    "        training_accuracy.append(clf.score(X_train, y_train))\n",
    "        testing_accuracy.append(clf.score(X_test, y_test))\n",
    "        number_of_neighbors.append(n_neighbors)\n",
    "        weighting_choice.append(weights)\n",
    "      \n",
    "     \n",
    "combinations_sorted_knn = sorted(list(zip(number_of_neighbors, weighting_choice, training_accuracy, testing_accuracy)), key = lambda e:e[3], reverse = True)\n",
    "\n",
    "print('Top 5 results, sorted by test accuracy:\\n')\n",
    "print(*combinations_sorted_knn[0:5], sep = \"\\n\")\n",
    "\n",
    "# save the best variables\n",
    "knn_best_n_neighbors = combinations_sorted_knn[0][0]\n",
    "knn_best_weights = combinations_sorted_knn[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.1 KNN - Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We define the parameters which want to test for the KNN model\n",
    "param_grid = {\n",
    "    \"n_neighbors\": [32,54,74,100], \"weights\": [\"uniform\", \"distance\"], \"metric\": [\"euclidean\", \"manhatten\"], \"leaf_size\": [10,30,60]\n",
    "}\n",
    "\n",
    "#We make the model with cross validation and grid search\n",
    "grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=10)\n",
    "\n",
    "#We fit the model, with the best parameters: \n",
    "gs_result = grid_search.fit(X_train, y_train)\n",
    "\n",
    "#print the results: \n",
    "\n",
    "print(\n",
    "    \"\\n Training score: \", gs_result.best_score_, \n",
    "    \"\\n Best estimator: \", gs_result.best_estimator_\n",
    "    # ,\"\\n best parameters: \", gs_result.best_params_\n",
    ")\n",
    "\n",
    "print(\"Test score: \", grid_search.score(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Play around with Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = []\n",
    "test_acc = []\n",
    "alpha_value = []\n",
    "learning_rate_value = []\n",
    "\n",
    "\n",
    "alphas = (0.0001, 0.001, 0.1, 0, 1, 5, 100)\n",
    "learning_rates = ('constant', 'invscaling', 'adaptive')\n",
    "\n",
    "for a in alphas:\n",
    "    for l in learning_rates:\n",
    "        mpl = MLPClassifier(alpha=a, learning_rate = l)\n",
    "        mpl.fit(X_train, y_train)\n",
    "        train_acc.append(accuracy_score(mpl.predict(X_train), y_train))\n",
    "        test_acc.append(accuracy_score(mpl.predict(X_test), y_test))\n",
    "        alpha_value.append(a)\n",
    "        learning_rate_value.append(l)\n",
    "        \n",
    "\n",
    "combinations_sorted_mpl = sorted(list(zip(alpha_value, learning_rate_value, train_acc, test_acc)), key = lambda e:e[3], reverse = True)\n",
    "\n",
    "print('Top 5 results, sorted by test accuracy:\\n')\n",
    "print(*combinations_sorted_mpl[0:5], sep = \"\\n\")\n",
    "\n",
    "# save the best variables\n",
    "mpl_best_alpha = combinations_sorted_knn[0][0]\n",
    "mpl_best_learning_rate = combinations_sorted_knn[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network - Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We define the parameters which want to test for the decision tree model\n",
    "param_grid = {\"learning_rate\":[\"constant\", \"invscaling\", \"adaptive\"], \"alpha\":[0.01,0.01,1,5,10,15]}\n",
    "\n",
    "#We make the model with cross validation and grid search\n",
    "grid_search = GridSearchCV(MLPClassifier(), param_grid, cv=10)\n",
    "\n",
    "#We fit the model, with the best parameters: \n",
    "gs_result = grid_search.fit(X_train, y_train)\n",
    "\n",
    "#print the results: \n",
    "\n",
    "print(\n",
    "    \"\\n Training score: \", gs_result.best_score_, \n",
    "    \"\\n Best estimator: \", gs_result.best_estimator_,\n",
    "    \"\\n best parameters: \", gs_result.best_params_,\n",
    "    \"Test score: \", grid_search.score(X_test,y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Play around with Decision Tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = []\n",
    "test_acc = []\n",
    "max_depth_value = []\n",
    "\n",
    "for i in range(1,30):\n",
    "    dt = DecisionTreeClassifier(max_depth = i, random_state=0)\n",
    "    dt.fit(X_train, y_train)\n",
    "    train_acc.append(accuracy_score(dt.predict(X_train), y_train))\n",
    "    test_acc.append(accuracy_score(dt.predict(X_test), y_test))\n",
    "    max_depth_value.append(i)\n",
    "\n",
    "combinations_sorted_tree = sorted(list(zip(max_depth_value, train_acc, test_acc)), key = lambda e:e[2], reverse = True)\n",
    "\n",
    "print('Top 5 results, sorted by test accuracy:\\n   (Values: depth, training accracy, test accuracy)\\n')\n",
    "print(*combinations_sorted_tree[0:5], sep = \"\\n\")\n",
    "\n",
    "# save the best variable\n",
    "tree_best_max_depth = combinations_sorted_tree[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.6.1 Decision Tree - Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We define the parameters which want to test for the decision tree model\n",
    "param_grid = {'criterion':['gini','entropy'],'max_depth':[1,2,3,4,5,6,7,8,9,10,12,15,20]}\n",
    "\n",
    "#We make the model with cross validation and grid search\n",
    "grid_search = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=10)\n",
    "\n",
    "#We fit the model, with the best parameters: \n",
    "gs_result = grid_search.fit(X_train, y_train)\n",
    "\n",
    "#print the results: \n",
    "\n",
    "print(\n",
    "    \"\\n Training score: \", gs_result.best_score_, \n",
    "    \"\\n Best estimator: \", gs_result.best_estimator_,\n",
    "    \"\\n best parameters: \", gs_result.best_params_,\n",
    "    \"Test score: \", grid_search.score(X_test,y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7 Play around with Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = []\n",
    "test_acc = []\n",
    "max_depth_value = []\n",
    "criterion_value = []\n",
    "max_features_value = []\n",
    "\n",
    "criterions = ('gini', 'entropy')\n",
    "number_of_features = range(1,6)\n",
    "\n",
    "for i in range(1,9):\n",
    "    for c in criterions:\n",
    "        for f in number_of_features:\n",
    "                rf = RandomForestClassifier(criterion = c, max_depth = i, max_features = f, random_state=0)\n",
    "                rf.fit(X_train, y_train)\n",
    "                train_acc.append(accuracy_score(rf.predict(X_train), y_train))\n",
    "                test_acc.append(accuracy_score(rf.predict(X_test), y_test))\n",
    "                max_depth_value.append(i)\n",
    "                criterion_value.append(c)\n",
    "                max_features_value.append(f)\n",
    "\n",
    "combinations_sorted_forest = sorted(list(zip(max_features_value, criterion_value, max_depth_value, train_acc, test_acc)), key = lambda e:e[3], reverse = True)\n",
    "\n",
    "print('Top 5 results, sorted by test accuracy:\\n   (Values: depth, training accracy, test accuracy)\\n')\n",
    "print(*combinations_sorted_forest[0:5], sep = \"\\n\")\n",
    "\n",
    "# save the best variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest - Cross validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We define the parameters which want to test for the decision tree model\n",
    "param_grid = {\n",
    "    \"max_features\":[2,4,6,8,10,12,20,40], 'max_depth':[1,2,3,4,5,6,7,8,9,10,12,15,20], \"criterion\":['gini', 'entropy']\n",
    "             }\n",
    "\n",
    "#We make the model with cross validation and grid search\n",
    "grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=10)\n",
    "\n",
    "#We fit the model, with the best parameters: \n",
    "gs_result = grid_search.fit(X_train, y_train)\n",
    "\n",
    "#print the results: \n",
    "\n",
    "print(\n",
    "    \"\\n Training score: \", gs_result.best_score_, \n",
    "    \"\\n Best estimator: \", gs_result.best_estimator_,\n",
    "    \"\\n best parameters: \", gs_result.best_params_,\n",
    "    \"Test score: \", grid_search.score(X_test,y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.8 Look at the best combinations with each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('KNN:', combinations_sorted_knn[0])\n",
    "print('Neural Network:', combinations_sorted_mpl[0])\n",
    "print('Decision Tree:', combinations_sorted_tree[0])\n",
    "print('Random Forest:', combinations_sorted_forest[0])\n",
    "\n",
    "# for now, the KNN model has the highest test accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Making a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Getting a prediction set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not working right now because all features are used in the training set. that means that the user should enter all ~250 values here, too\n",
    "# once we decide what features are the most influential, we can make this work, too\n",
    "\n",
    "user_inputs = []\n",
    "input_names = ['R_Height_cms', 'B_Height_cms', 'R_Reach_cms', 'B_Reach_cms', 'R_Weight_lbs', 'B_Weight_lbs', 'R_age', 'B_age', 'R_current_win_streak', 'B_current_win_streak', 'R_avg_SIG_STR_landed', 'B_avg_SIG_STR_landed']\n",
    "\n",
    "\n",
    "for i in input_names:\n",
    "    print(i, ':')\n",
    "    user_inputs.append(input())\n",
    "    print('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
